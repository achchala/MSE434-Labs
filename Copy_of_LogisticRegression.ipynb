{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/achchala/MSE434-Labs/blob/main/Copy_of_LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "metadata": {
        "id": "p-QgxQAV6JCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(n):\n",
        "    \"\"\"\n",
        "    Generates data for a logistic regression problem\n",
        "\n",
        "    Parameters:\n",
        "    n: number of data\n",
        "\n",
        "    Returns:\n",
        "    X (numpy array): input features\n",
        "    y (numpy array): target values\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(0) # set the seed for reproducibility\n",
        "    X = np.random.randn(n, 2) # generate random values with normal distribution with mean 0 and standard deviation 1\n",
        "    y = np.zeros((n, 1)) # initialize y with all zeros\n",
        "    y[X[:, 1] > X[:, 0]] = 1 # assign 1 to the elements in y where the second column of X is greater than the first column\n",
        "    X = X + np.random.normal(0, 0.1, X.shape) # add random noise with normal distribution with mean 0 and standard deviation 0.1\n",
        "    return X, y # return the generated data\n",
        "\n",
        "# n is set to 100\n",
        "n = 100\n",
        "X, y = generate_data(n) # call the function with n as the input"
      ],
      "metadata": {
        "id": "8Lj5pfkb6PFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Calculates the sigmoid function for the given input z\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def logistic_regression_gd(X, y, learning_rate=0.1, num_iters=1000):\n",
        "    \"\"\"\n",
        "    Implements the logistic regression model using gradient descent\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy array): input features\n",
        "    y (numpy array): target values\n",
        "    learning_rate (float): the learning rate for the gradient descent algorithm\n",
        "    num_iters (int): number of iterations for the gradient descent algorithm\n",
        "\n",
        "    Returns:\n",
        "    numpy array: optimized weights\n",
        "    \"\"\"\n",
        "    n = len(X)\n",
        "    # Add a column of ones to X for the intercept term\n",
        "    X = np.hstack((np.ones((n, 1)), X))\n",
        "    # Initialize theta with zeros\n",
        "    theta = np.zeros((X.shape[1], 1))\n",
        "    m = X.shape[0]\n",
        "    # Loop over the number of iterations\n",
        "    for i in range(num_iters):\n",
        "        # Calculate the dot product of X and theta\n",
        "        z = X.dot(theta)\n",
        "\n",
        "        ## Add your solution here ##.\n",
        "        # Calculate the gradient\n",
        "        # Update theta\n",
        "\n",
        "    return theta\n",
        "\n",
        "def plot_decision_boundary_gd(theta, X, y):\n",
        "    \"\"\"\n",
        "    Plots the decision boundary for the logistic regression model\n",
        "\n",
        "    Parameters:\n",
        "    theta (numpy array): optimized weights\n",
        "    X (numpy array): input features\n",
        "    y (numpy array): target values\n",
        "    \"\"\"\n",
        "    n = len(X)\n",
        "    # Generate a grid of points for x1\n",
        "    x1 = np.linspace(X[:, 0].min(), X[:, 0].max(), n)\n",
        "    # Generate a grid of points for x2\n",
        "    x2 = np.linspace(X[:, 1].min(), X[:, 1].max(), n)\n",
        "    # Create a meshgrid from x1 and x2\n",
        "    X1, X2 = np.meshgrid(x1, x2)\n",
        "    # Stack the meshgrid into a single array\n",
        "    X_test = np.hstack((X1.reshape(-1, 1), X2.reshape(-1, 1)))\n",
        "    # Add a column of ones to X_test for the intercept term\n",
        "    X_test = np.hstack((np.ones((n**2, 1)), X_test))\n",
        "    # Calculate the dot product of X_test and theta\n",
        "    y_pred = X_test.dot(theta)\n",
        "    # Apply the sigmoid function to y_pred\n",
        "    y_pred = np.round(1 / (1 + np.exp(-y_pred)))\n",
        "    # Reshape y_pred into a grid\n",
        "    y_pred = y_pred.reshape(n, n)\n",
        "     # Plot the decision boundary using the contour function\n",
        "\n",
        "    plt.contourf(X1, X2, y_pred, cmap='binary')\n",
        "\n",
        "    # Plot the data points as scatterplots\n",
        "    plt.scatter(X[y[:, 0] == 0, 0], X[y[:, 0] == 0, 1], c='r')\n",
        "    plt.scatter(X[y[:, 0] == 1, 0], X[y[:, 0] == 1, 1], c='b')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "ug_SBsKTStmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta_gd = logistic_regression_gd(X, y) # Call the function with X and y as inputs\n",
        "plot_decision_boundary_gd(theta_gd, X, y) # Plot the decision boundary"
      ],
      "metadata": {
        "id": "0RiYV0bB_ZzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Logistic Regression using Stochastic Gradient Descent\n",
        "\n",
        "def logistic_regression_sgd(X, y, learning_rate=0.1, num_iters=1000):\n",
        "    \"\"\"\n",
        "    Implements the logistic regression model using stochastic gradient descent\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy array): input features\n",
        "    y (numpy array): target values\n",
        "    learning_rate (float): the learning rate for the stochastic gradient descent algorithm\n",
        "    num_iters (int): number of iterations for the stochastic gradient descent algorithm\n",
        "\n",
        "    Returns:\n",
        "    numpy array: optimized weights\n",
        "    \"\"\"\n",
        "    n = len(X)\n",
        "    # Add a column of ones to X for the intercept term\n",
        "    X = np.hstack((np.ones((n, 1)), X))\n",
        "    # Initialize theta with zeros\n",
        "    theta = np.zeros((X.shape[1], 1))\n",
        "    m = X.shape[0]\n",
        "    # Loop over the number of iterations\n",
        "    for i in range(num_iters):\n",
        "        # Select a random sample\n",
        "        index = np.random.randint(0, m)\n",
        "        x = X[index, :].reshape(1, X.shape[1])\n",
        "        y_ = y[index, :].reshape(1, 1)\n",
        "        # Calculate the dot product of x and theta\n",
        "        z = x.dot(theta)\n",
        "\n",
        "        ## Add your solution here ##.\n",
        "        # Calculate the gradient\n",
        "        # Update theta\n",
        "\n",
        "    return theta\n",
        "\n",
        "\n",
        "def plot_decision_boundary_sgd(theta, X, y):\n",
        "    \"\"\"\n",
        "    Plots the decision boundary for the logistic regression model for stochastic gradient descent\n",
        "\n",
        "    Parameters:\n",
        "    theta (numpy array): optimized weights\n",
        "    X (numpy array): input features\n",
        "    y (numpy array): target values\n",
        "    \"\"\"\n",
        "    n = len(X)\n",
        "    # Generate a grid of points for x1\n",
        "    x1 = np.linspace(X[:, 0].min(), X[:, 0].max(), n)\n",
        "    # Generate a grid of points for x2\n",
        "    x2 = np.linspace(X[:, 1].min(), X[:, 1].max(), n)\n",
        "    # Create a meshgrid from x1 and x2\n",
        "    X1, X2 = np.meshgrid(x1, x2)\n",
        "    # Stack the meshgrid into a single array\n",
        "    X_test = np.hstack((X1.reshape(-1, 1), X2.reshape(-1, 1)))\n",
        "    # Add a column of ones to X_test for the intercept term\n",
        "    X_test = np.hstack((np.ones((n**2, 1)), X_test))\n",
        "    # Calculate the dot product of X_test and theta\n",
        "    y_pred = X_test.dot(theta)\n",
        "    # Apply the sigmoid function to y_pred\n",
        "    y_pred = np.round(1 / (1 + np.exp(-y_pred)))\n",
        "    # Reshape y_pred into a grid\n",
        "    y_pred = y_pred.reshape(n, n)\n",
        "     # Plot the decision boundary using the contour function\n",
        "\n",
        "    plt.contourf(X1, X2, y_pred, cmap='binary')\n",
        "\n",
        "    # Plot the data points as scatterplots\n",
        "    plt.scatter(X[y[:, 0] == 0, 0], X[y[:, 0] == 0, 1], c='r')\n",
        "    plt.scatter(X[y[:, 0] == 1, 0], X[y[:, 0] == 1, 1], c='b')\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "qjdEp3k8ZNY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theta_sgd = logistic_regression_sgd(X, y) # Call the function with X and y as inputs\n",
        "plot_decision_boundary_gd(theta_sgd, X, y) # Plot the decision boundary"
      ],
      "metadata": {
        "id": "P1QU6AN7_bQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_error(theta, X, y):\n",
        "    \"\"\"\n",
        "    Calculates the logistic regression error between the predicted and actual target values\n",
        "\n",
        "    Parameters:\n",
        "    theta (numpy array): optimized weights\n",
        "    X (numpy array): input features\n",
        "    y (numpy array): target values\n",
        "\n",
        "    Returns:\n",
        "    float: error value\n",
        "    \"\"\"\n",
        "    # Number of examples\n",
        "    n = len(X)\n",
        "    # Add a column of ones to X for the intercept term\n",
        "    X = np.hstack((np.ones((n, 1)), X))\n",
        "    # Calculate the dot product of X and theta\n",
        "    z = np.dot(X, theta)\n",
        "    # Calculate the predicted target values\n",
        "    h = sigmoid(z)\n",
        "    # Calculate the logistic regression error between h and y\n",
        "    error = -np.mean(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
        "    return error\n"
      ],
      "metadata": {
        "id": "3SwBpupW9Iqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the time and accuracy of each implementation of logistic regression\n",
        "\n",
        "start_time = time.time()\n",
        "theta_gd = logistic_regression_gd(X, y)\n",
        "print(\"Time taken by Gradient Descent:\", time.time() - start_time, \"Error of Gradient Descent:\", calculate_error(theta_gd, X, y))\n",
        "\n",
        "start_time = time.time()\n",
        "theta_sgd = logistic_regression_sgd(X, y)\n",
        "print(\"Time taken by Stochastic Gradient Descent:\", time.time() - start_time, \"Error of Stochastic Gradient Descent:\", calculate_error(theta_sgd, X, y))"
      ],
      "metadata": {
        "id": "zBmCP-Sj8kJh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}